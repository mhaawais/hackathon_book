# Data Model: Docusaurus URL Ingestion, Embedding & Vector Storage

## Overview
Data model defining the entities and relationships for the Docusaurus ingestion pipeline.

## Key Entities

### Book Content Chunk
**Description**: Represents a segment of extracted book text with semantic coherence

**Fields**:
- `chunk_id` (string): Deterministic identifier generated from URL and content hash
- `url` (string): Original URL of the source page
- `section` (string): Section or chapter name from the book hierarchy
- `heading` (string): Primary heading of the content chunk
- `content` (string): Clean extracted text content
- `created_at` (datetime): Timestamp when chunk was first created
- `updated_at` (datetime): Timestamp when chunk was last updated
- `metadata` (dict): Additional metadata from the source page

**Validation Rules**:
- `chunk_id` must be unique across all chunks
- `url` must be a valid URL format
- `content` must not be empty
- `content` length should be within chunk size limits

### Vector Embedding
**Description**: Mathematical representation of content chunk generated by Cohere API

**Fields**:
- `embedding_id` (string): Reference to the associated content chunk ID
- `vector` (list[float]): High-dimensional vector representation from Cohere
- `dimension` (int): Dimensionality of the embedding vector
- `model_version` (string): Version of the Cohere model used
- `generated_at` (datetime): Timestamp when embedding was generated

**Validation Rules**:
- `embedding_id` must reference an existing content chunk
- `vector` must have consistent dimensionality
- `dimension` must match the expected model output

### Ingestion Job
**Description**: Represents a complete ingestion process run

**Fields**:
- `job_id` (string): Unique identifier for the ingestion job
- `status` (string): Current status (pending, running, completed, failed)
- `source_url` (string): Base URL of the Docusaurus book being ingested
- `pages_processed` (int): Number of pages successfully processed
- `chunks_created` (int): Number of content chunks created
- `errors` (list): List of errors encountered during processing
- `started_at` (datetime): Timestamp when job started
- `completed_at` (datetime): Timestamp when job completed

**State Transitions**:
- `pending` → `running`: When ingestion begins
- `running` → `completed`: When all pages are processed successfully
- `running` → `failed`: When critical errors occur

## Relationships

- One `Ingestion Job` contains many `Book Content Chunks`
- One `Book Content Chunk` has one `Vector Embedding`
- Many `Book Content Chunks` may reference the same source URL

## Data Flow

1. `Ingestion Job` is initiated with a source URL
2. Pages are crawled and converted to multiple `Book Content Chunks`
3. Each chunk generates a `Vector Embedding`
4. Both chunks and embeddings are stored in Qdrant Cloud
5. Job status is updated with processing results